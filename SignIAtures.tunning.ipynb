{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.2\n",
    "Fine-tuning\n",
    "Another widely used technique for model reuse, complementary to feature\n",
    "extraction, is fine-tuning (see figure 5.19). Fine-tuning consists of unfreezing a few of\n",
    "the top layers of a frozen model base used for feature extraction, and jointly training\n",
    "both the newly added part of the model (in this case, the fully connected classifier)\n",
    "and these top layers. This is called fine-tuning because it slightly adjusts the more\n",
    "abstract representations of the model being reused, in order to make them more rele-\n",
    "vant for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to\n",
    "be able to train a randomly initialized classifier on top. For the same reason, it’s only\n",
    "possible to fine-tune the top layers of the convolutional base once the classifier on top\n",
    "has already been trained. If the classifier isn’t already trained, then the error signal\n",
    "propagating through the network during training will be too large, and the represen-\n",
    "tations previously learned by the layers being fine-tuned will be destroyed. Thus the\n",
    "steps for fine-tuning a network are as follow:\n",
    "    1.  Add your custom network on top of an already-trained base network.\n",
    "    2.  Freeze the base network.\n",
    "        Listing 5.22 Freezing all layers up to a specific one\n",
    "        >>>conv_base.trainable = True\n",
    "        >>>set_trainable = False\n",
    "        >>>for layer in conv_base.layers:\n",
    "        >>>    if layer.name == 'block5_conv1':\n",
    "        >>>        set_trainable = True\n",
    "        >>>    if set_trainable:\n",
    "        >>>        layer.trainable = True\n",
    "        >>>    else:\n",
    "        >>>        layer.trainable = False\n",
    "    3.  Train the part you added.\n",
    "    4.  Unfreeze some layers in the base network.\n",
    "    5.  Jointly train both these layers and the part you added.\n",
    "You already completed the first three steps when doing feature extraction. Let’s pro-\n",
    "ceed with step 4: you’ll unfreeze your conv_base and then freeze individual layers\n",
    "inside it.\n",
    "# As a reminder, this is what your convolutional base looks like:\n",
    "# >>> conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll fine-tune the last three convolutional layers, which means all layers up to\n",
    "block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and\n",
    "block5_conv3 should be trainable.\n",
    "Why not fine-tune more layers? Why not fine-tune the entire convolutional base?\n",
    "You could. But you need to consider the following:\n",
    " Earlier layers in the convolutional base encode more-generic, reusable features,\n",
    "whereas layers higher up encode more-specialized features. It’s more useful to\n",
    "fine-tune the more specialized features, because these are the ones that need to\n",
    "be repurposed on your new problem. There would be fast-decreasing returns in\n",
    "fine-tuning lower layers.\n",
    " The more parameters you’re training, the more you’re at risk of overfitting.\n",
    "The convolutional base has 15 million parameters, so it would be risky to\n",
    "attempt to train it on your small dataset.\n",
    "Thus, in this situation, it’s a good strategy to fine-tune only the top two or three layers\n",
    "in the convolutional base. Let’s set this up, starting from where you left off in the pre-\n",
    "vious example.\n",
    "\n",
    "# >>>model.compile(loss='binary_crossentropy',\n",
    "# >>>optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "# >>>metrics=['acc'])\n",
    "# >>>history = model.fit_generator(\n",
    "# >>>train_generator,\n",
    "# >>>steps_per_epoch=100,\n",
    "# >>>epochs=100,\n",
    "# >>>validation_data=validation_generator,\n",
    "# >>>validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
